{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618c858e-f8fc-4069-91f5-0d55ba7a0852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StaticDataFrame\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b898d14-a151-4ea7-83e4-892d4500f839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a sales_df static dataframe from the sales.csv file.\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"seller_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),   \n",
    "    StructField(\"num_pieces_sold\", IntegerType(), True),\n",
    "    StructField(\"bill_raw_text\", StringType(), True),\n",
    "])\n",
    "\n",
    "sales_df = spark.read.format(\"csv\") \\\n",
    "                 .option(\"header\", True) \\\n",
    "                 .schema(sales_schema) \\\n",
    "                 .load(\"sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a3299d-0de5-4600-9435-b47d31408f97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000040\n"
     ]
    }
   ],
   "source": [
    "# Verify that count of sales_df > 4 million records.\n",
    "if sales_df.count() > 4000000:\n",
    "    print(sales_df.count())\n",
    "else:\n",
    "    print(\"Count < 4 000 000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c61986-228b-42ca-8ebf-0b4cb588998f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|   95016|     10292|        7|2020-07-07|             86|ddjxhepqkpwionxlw...|\n",
      "|   95023|     13642|        7|2020-07-03|              5|hqlmpvnyluuvdxNyl...|\n",
      "|   95042|     54476|        7|2020-07-10|             75|frecsxijhiwsujzuu...|\n",
      "|   95065|     86757|        7|2020-07-04|             95|dxrsOgtaxcxcpulcc...|\n",
      "|   95067|      9749|        7|2020-07-06|             83|unhtoxghdrgvvtemo...|\n",
      "|   95070|     88175|        7|2020-07-09|             32|azfydzqrzsftwhzqy...|\n",
      "|   95075|     36251|        7|2020-07-09|             56|rrezcdybsktrdfuvr...|\n",
      "|   95089|     73423|        7|2020-07-10|             15|noqsnpzdtiobzqqlu...|\n",
      "|   95092|     57982|        7|2020-07-06|             94|tkaviwhjZlzrtviqq...|\n",
      "|   95096|      3694|        7|2020-07-02|             19|wymacbkmqednyybou...|\n",
      "|   95105|     51200|        7|2020-07-08|             54|mjzkjszsyzokbusqr...|\n",
      "|   95112|     96554|        7|2020-07-06|             84|iheixykavfjngybzf...|\n",
      "|   95118|     88483|        7|2020-07-10|             76|bnfalaawuyftzwpvv...|\n",
      "|   95127|     29759|        7|2020-07-07|             84|snpkpcqlubgjwvpax...|\n",
      "|   95128|     68914|        7|2020-07-02|             94|yawuplskokrutoyer...|\n",
      "|   95129|     73927|        7|2020-07-04|             62|ttxamfaapqnoyiyfc...|\n",
      "|   95151|     63000|        7|2020-07-08|             51|kfreekpblnblejxvv...|\n",
      "|   95155|     61577|        7|2020-07-04|            100|zrfskjibcmrbotcyx...|\n",
      "|   95161|     85462|        7|2020-07-02|             51|kyqygdvabiswgyvxa...|\n",
      "|   95164|     95701|        7|2020-07-01|            100|dgvbosbqgrpelnohq...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a separate static dataframe for all sales of the seller with id=7.\n",
    "\n",
    "sales_seller_7_df = sales_df.filter(sales_df.seller_id == 7)\n",
    "sales_seller_7_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a325b0bb-3970-421f-9575-e4d0efe73d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the empty “input_data” folder.\n",
    "\n",
    "if not os.path.exists(\"input_data\"):\n",
    "    os.makedirs(\"input_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4281994e-fe71-4db5-a3bb-7ac90324f794",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create an input streaming dataframe for csv files in the input_data folder. \n",
    "# Make a deduplication by the “order_id” column. \n",
    "# Apply an aggregation function to calculate sum of the num_pieces_sold. \n",
    "\n",
    "input_streaming_df = spark.readStream.format(\"csv\") \\\n",
    "                        .option(\"header\", True) \\\n",
    "                        .schema(sales_schema) \\\n",
    "                        .option(\"maxFilesPerTrigger\", 1) \\\n",
    "                        .load(\"input_data\") \\\n",
    "\t\t\t\t\t\t.dropDuplicates([\"order_id\"]) \\\n",
    "                        .withColumn(\"date\", col(\"date\").cast(TimestampType())) \\\n",
    "                        .withWatermark(\"date\", \"1 day\") \\\n",
    "                        .groupBy(\"date\") \\\n",
    "                        .agg(sum(\"num_pieces_sold\").alias(\"total_num_pieces_sold\"))\n",
    "\n",
    "# Verify that the created object is streaming.\n",
    "print(input_streaming_df.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c0d9c6-c37d-4a97-a93f-457669bb7cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# Create an output csv streaming sink to the output_data folder. \n",
    "# Apply the partitioning by date. Add the 10 seconds micro-batch interval. \n",
    "\n",
    "query = input_streaming_df.writeStream \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .format(\"csv\") \\\n",
    "            .option(\"path\", \"output_data\") \\\n",
    "            .option(\"checkpointLocation\", \"checkpoint_location\") \\\n",
    "            .partitionBy(\"date\") \\\n",
    "            .trigger(processingTime=\"10 seconds\") \\\n",
    "            .start()\n",
    "# Get the current status of the object.\n",
    "print(query.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea53dd6-a639-49c2-867f-80a516a30c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write a static dataframe from the task 1.3 to the “input_data” folder.\n",
    "\n",
    "sales_seller_7_df.write.mode('overwrite').format(\"csv\") \\\n",
    "                    .option(\"header\", True) \\\n",
    "                    .option(\"path\", \"input_data/\") \\\n",
    "                    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1e6e13-fbd1-4de9-a5cc-8929d9b15962",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_temperature_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the output sink to memory with 30 minutes trigger\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m memory_sink \u001b[38;5;241m=\u001b[39m (\u001b[43mavg_temperature_df\u001b[49m\n\u001b[1;32m      3\u001b[0m                \u001b[38;5;241m.\u001b[39mwriteStream\n\u001b[1;32m      4\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m                \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_temperature\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m                \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m30 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m                \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      8\u001b[0m               )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_temperature_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f4ee8-04bf-452d-b942-61b3db9fc5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
